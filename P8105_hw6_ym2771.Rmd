---
title: "hw6"
author: "Yuqi Miao"
date: "11/16/2019"
output: github_document
---

```{r include=FALSE}
library(tidyverse)
library(broom)
library(readxl)
library(viridis)
library(modelr)
library(patchwork)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis")

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

## data cleaning

```{r}
birthweight <- read_csv(file = "data/birthweight.csv") %>% 
    janitor::clean_names() %>% 
    mutate(
        babysex = factor(babysex,levels = c(1,2), labels =c("male","female")),
        frace = factor(frace, levels = c(1,2,3,4,8,9),labels = c("White","Black","Asian","Puerto Rican","Others","Unknown")),
        malform = factor(malform, levels = c(0,1), labels = c("absent","present")),
        mrace = factor(mrace, levels = c(1,2,3,4,8,9),labels = c("White","Black","Asian","Puerto Rican","Others","Unknown"))
    )

```

## building data driven model


```{r}
# birthweight %>%
#     select_if(is.numeric) %>% ## Only select continuous var
#     select(-pnumlbw,-pnumsga,-parity) %>% ## rule out 2 columns with only # 0:pnumlbw,pnumsga,parity
#     mutate_if(is.numeric, list(scale)) %>% ## scale the variables
#     pivot_longer(cols = -bwt, names_to = "factor",values_to = "value") %>% 
#     ggplot(aes(x = value, y = bwt,color = factor)) + 
#     geom_point(alpha = 0.5, size = 0.5)+
#     geom_smooth(method = "lm",lwd = 0.2) +
#     scale_x_continuous(limits = c(0,2))+
#     facet_grid(.~factor)
 ## Because the predictors and outcome have different scales, firstly, I use scale to make every variable in (0,1), then filter the ones with only (or mostly 0),which is `pnumlbw`,`pnumsga`,`parity`,According to the simple linear regreassion results shown in the plot, choosing `bhead`,`blength`,`delwt`,`mheight`,`wtgain` as my model predictor.
### making a overal mlr
```

### overall regression among all variables.
```{r}
mlr_all=lm(bwt~.,data = birthweight)
summary(mlr_all)
```
Firstly, I put all variables in the model, and based on the coefficient significance test, we choose variables with p-value of coefficient smaller than 0.005. Based on the overall regression, choosing the significant predictors to build a model, which includes `babysex`,`bhead`,`blength`,`delwt`,`gaweeks`,`mrace`,`smoke`. As shown in the summary table, the Adjusted R-squared has come up to 0.717, which shows relatively great performance.

  

### model building

```{r}
mlr_1 <- birthweight %>% 
    mutate(
        mrace = fct_infreq(mrace),
        babysex = fct_infreq(babysex)
        ) %>% 
    lm(bwt~babysex+bhead+blength+delwt+mrace+gaweeks+smoken,data = .)
summary(mlr_1)

```
### add the fitted value and residuals
```{r}
birthweight %>% 
    modelr::add_residuals(mlr_1) %>% 
    modelr::add_predictions(mlr_1) %>% 
    ggplot(aes(y = resid, x = pred)) +
    geom_point(color = "darkblue", alpha = 0.5,size = 0.5)+
    geom_smooth(method = "lm")+
    geom_line(aes(y = 0), linetype = "dashed")
```

As shown in plot, the residual and fitted value has no relationship, though the residuals shows reletively constant variance, the magnitute of residuals is relatively high, indicating high variance of the model.

## Comparing models

```{r}
mlr_2 <- lm(bwt~blength+ gaweeks,data = birthweight)
mlr_3 <- lm(bwt~bhead*blength*babysex, data = birthweight)
summary(mlr_3)
```


### Cross validation comparision
```{r}
cv_bwt <- 
    crossv_mc(birthweight,100) %>% 
    mutate(
        train = map(train, as.tibble),
        test = map(test,as.tibble)
    ) 
cv_bwt = cv_bwt %>% 
    mutate(
        model1 = map(train,~lm(bwt~babysex+bhead+blength+delwt+mrace+gaweeks+smoken,data = .x)),
        model2 = map(train,~lm(bwt~blength+ gaweeks,data = .x)),
        model3 = map(train,~lm(bwt~bhead*blength*babysex, data = .x))
    ) %>% 
    mutate(
        rmse_model1=map2_dbl(model1, test,~rmse(model = .x,data = .y)),
        rmse_model2=map2_dbl(model2, test,~rmse(model = .x,data = .y)),
        rmse_model3=map2_dbl(model3, test,~rmse(model = .x,data = .y))
    )
cv_bwt %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "models",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  mutate(
    models = fct_reorder(models,rmse)
  ) %>% 
  ggplot(aes(x = models, y = rmse))+
  geom_violin()
  
    
```

## comment
Here we make comparison between 3 models: 

* model1(self-builted) is stated above

* model2 takes the main effect of `blength` + `gaweeks` 

* model 3 is the saturated model of `bhead`,`blength` and `babysex`

Based on the the plot above, model1 shows the best performance, and model2 which only includes main effect, shows the worst performance.

# Problem 2

## getting data

```{r}
weather_df =
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"),
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark
_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

## Bootstrap samples

```{r}
boot_wd <- 
  weather_df %>% 
  bootstrap(5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax~tmin,data = .x)),
    results = map(models,broom::tidy),
    rsquare = map(models, broom::glance)
  ) 
boot_wd <- 
  boot_wd%>% 
  select(-models,-strap) %>% 
  unnest(results,rsquare) 

g1=
  boot_wd %>% 
  group_by(.id) %>% 
  summarise(r_square = mean(r.squared)) %>% 
  ggplot(aes(x = r_square)) +
  geom_density(fill = "lightblue", color = "darkblue")
  

g2=
  boot_wd %>% 
  select(.id,term,estimate) %>% 
  pivot_wider(id_cols = .id,names_from =term,values_from = estimate) %>%
  mutate(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_beta_product = log(beta0*beta1)) %>% 
  ggplot(aes(x = log_beta_product)) +
  geom_density(fill = "lightblue", color = "darkblue")
g1+g2
```

### comment
Based on the plot above, the distribution of these two parameters are nearly normal with slight left skewness and relatively heavy tails. 


## counstruct CI for parameters.

```{r}
CI_rsquare <- 
  boot_wd %>% 
  group_by(.id) %>% 
  summarise(r_square = mean(r.squared)) %>% 
  ungroup() %>% 
  summarise(upr = quantile(r_square,0.025),lwr = quantile(r_square,0.975))



CI_log_beta_product <- 
  boot_wd %>% 
  select(.id,term,estimate) %>% 
  pivot_wider(id_cols = .id,names_from =term,values_from = estimate) %>%
  mutate(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_beta_product = log(beta0*beta1)) %>% 
  summarise(upr = quantile(log_beta_product,0.025),lwr = quantile(log_beta_product,0.975))

CI_table <- 
  bind_rows(CI_log_beta_product,CI_rsquare) %>% 
  mutate(category = c("log_beta_product","rsquare"))

knitr::kable(CI_table, caption = "95% Confidence Interval for log_beta_product and r_square" )  
  
```




















